{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I202ixXcSqfh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGgOR6IsTnu-"
   },
   "outputs": [],
   "source": [
    "def roberts_sequence(\n",
    "        num_points,\n",
    "        num_dims,\n",
    "        root_iters=10_000,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates random numbers tiling a hybercube [0, 1]^d where d is `num_dims`.\n",
    "\n",
    "    Code modified from:\n",
    "    https://gist.github.com/carlosgmartin/1fd4e60bed526ec8ae076137ded6ebab\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the unique positive root of f using the Newton-Raphson method.\n",
    "    def f(x):\n",
    "        return x ** (num_dims + 1) - x - 1\n",
    "\n",
    "    def grad_f(x):\n",
    "        return (num_dims + 1) * (x ** num_dims) - 1\n",
    "\n",
    "    # Main loop.\n",
    "    x = 1.0\n",
    "    for i in range(root_iters):\n",
    "        x = x - f(x) / grad_f(x)\n",
    "\n",
    "    # Compute basis parameter\n",
    "    basis = 1 - (1 / x ** (1 + torch.arange(0, num_dims)))\n",
    "\n",
    "    # Return sequence without taking modulo 1\n",
    "    return torch.arange(0, num_points)[:, None] * basis[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        a, b = b, a+b\n",
    "    return a\n",
    "def gen_fib_basis(m):\n",
    "\n",
    "    n = fib(m)\n",
    "    z = torch.tensor([1.,fib(m-1)])\n",
    "\n",
    "    return torch.arange(0,n)[:,None]*z[None,:]/n\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_korobov_basis(a,d,n):\n",
    "    ## some recommended values:\n",
    "    ## n= 1021, a = 76\n",
    "    ## n = 2039, a = 1487\n",
    "    ## n = 4093, a = 1516\n",
    "    ## see table 16.1 of owens for more\n",
    "    ## these were constructed for d \\in {8,12,24,32}\n",
    "    ## this is a fibonacci lattice for d = 2, a = Fib(m-1), n = Fib(m) for m >= 3\n",
    "\n",
    "    z = torch.tensor([a**k % n for k in range(d)])\n",
    "    base_pts = torch.arange(0,n)[:,None] * z[None,:]/n\n",
    "    return base_pts#z[None,:]/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## korobov grid over 2d (from Art Owen Practical qMC integration 2023) \n",
    "d = 2\n",
    "n = 1021\n",
    "a = 76\n",
    "basis = gen_korobov_basis(a=a,d=d,n=n)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "for _ in range(6):\n",
    "    \n",
    "    pts = (torch.rand(1, d) + basis) % 1\n",
    "    #pts = base_pts % 1\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], lw=0, s=8)\n",
    "plt.show()\n",
    "plt.close()\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "#for _ in range(6):\n",
    "    \n",
    "#    pts = (torch.rand(1, d) + basis) % 1\n",
    "#    #pts = base_pts % 1\n",
    "#    ax.scatter(pts[:, 0], pts[:, 2], lw=0, s=8)\n",
    "#plt.show()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "9CZGQMZo1T4t",
    "outputId": "c7b202e2-39e7-4747-e60d-976812887a45"
   },
   "outputs": [],
   "source": [
    "m = 20\n",
    "## fibonacci grid over 2d (from Art Owen Practical qMC integration 2023) \n",
    "basis = gen_fib_basis(m)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    pts = (torch.rand(1, 2) + basis) % 1\n",
    "    #pts = base_pts % 1\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], lw=0, s=8)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDFYxQno0wTc"
   },
   "outputs": [],
   "source": [
    "n = 2 ** 8\n",
    "basis = roberts_sequence(n,num_dims=2)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "for _ in range(6):\n",
    "    pts = (torch.rand(1, 2) + basis) % 1\n",
    "    #pts = basis % 1\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], lw=0, s=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6UBJamMPefD"
   },
   "outputs": [],
   "source": [
    "class FourierBasis(nn.Module):\n",
    "\n",
    "    def __init__(self, num_dims=2, num_freqs=4, device=None):\n",
    "        super(FourierBasis, self).__init__()\n",
    "\n",
    "        # F.shape == (num_dims x num_basis_functions)\n",
    "        self.F = 2 * torch.pi * (\n",
    "            torch.stack(\n",
    "                torch.meshgrid(\n",
    "                    [torch.arange(num_freqs)] * num_dims, indexing=\"ij\"\n",
    "                )\n",
    "            ).reshape(\n",
    "                num_dims, num_freqs ** num_dims\n",
    "            )\n",
    "        ).to(device)\n",
    "        # self.wsin = nn.Parameter(torch.ones(num_freqs ** num_dims))\n",
    "        # self.wcos = nn.Parameter(torch.ones(num_freqs ** num_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape == (batch_size, num_dims)\n",
    "        \"\"\"\n",
    "        return torch.hstack(\n",
    "            (torch.sin(x @ self.F), torch.cos(x @ self.F))\n",
    "        )\n",
    "\n",
    "class QMCLVM(nn.Module):\n",
    "    def __init__(self, latent_dim=2, root_iters=100000, num_freqs=16, device=None):\n",
    "        super(QMCLVM, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fourier_basis = FourierBasis(\n",
    "            num_dims=latent_dim, num_freqs=num_freqs, device=device\n",
    "        )\n",
    "\n",
    "        # Decoder.\n",
    "        #nn.Unflatten(1, (64, 7, 7)),\n",
    "        #nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "        #nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "        #self.fourier_basis,\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim,2*num_freqs ** latent_dim),\n",
    "            nn.Linear(2 * num_freqs ** latent_dim, 64*7*7),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), #nn.Linear(64*7*7,32*14*14),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),#nn.Linear(32*14*14,1*28*28),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.Unflatten(1,(1,28,28))\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, eval_grid):\n",
    "        r = torch.rand(1, self.latent_dim, device=eval_grid.device)\n",
    "        x = (r + eval_grid) % 1\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def latent_density(self,eval_grid,sample,batch_size=-1):\n",
    "\n",
    "        decoded = self.decoder(eval_grid %1)\n",
    "        if batch_size == -1:\n",
    "            batch_size = sample.shape[0]\n",
    "\n",
    "        binom_lps = []\n",
    "        for on in range(0,sample.shape[0],batch_size):\n",
    "            off = on + batch_size\n",
    "            \n",
    "            s = sample[on:off].tile(1,decoded.shape[0],1,1)\n",
    "            d = decoded.swapaxes(0,1).tile(s.shape[0],1,1,1)\n",
    "            \n",
    "            binomLP = -1 * binary_cross_entropy(d,\n",
    "                                                s,\n",
    "                                                reduction='none'\n",
    "                                               ).sum(axis=(2,3))\n",
    "            binom_lps.append(binomLP)\n",
    "        binom_lps = torch.cat(binom_lps,axis=0)\n",
    "        return binom_lps\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNO8RUqUPvUW"
   },
   "outputs": [],
   "source": [
    "def loss_function(samples, data):\n",
    "    recon_loss = -1 * torch.mean(\n",
    "        torch.special.logsumexp(\n",
    "            torch.sum(\n",
    "                -1 * binary_cross_entropy(\n",
    "                    samples.swapaxes(0, 1).tile((data.shape[0], 1, 1, 1)),\n",
    "                    data.tile(1, samples.shape[0], 1, 1),\n",
    "                    reduction=\"none\"\n",
    "                ),\n",
    "                axis=(2, 3)\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "    return recon_loss\n",
    "\n",
    "# def loss_function(samples, data):\n",
    "#     return torch.mean(\n",
    "#         torch.special.logsumexp(\n",
    "#             torch.sum(\n",
    "#                 (samples.swapaxes(0, 1).tile((data.shape[0], 1, 1, 1)) - data.tile(1, samples.shape[0], 1, 1)) ** 2,\n",
    "#                 axis=(2, 3)\n",
    "#             ),\n",
    "#             axis=1\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = QMCLVM(latent_dim=2, num_freqs=16, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJITuDN3P2OV",
    "outputId": "ab2832a3-6142-4353-a562-61a3aede35ae"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(epoch, base_sequence):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(tqdm(train_loader)):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        samples = model(base_sequence)\n",
    "        loss = loss_function(samples, data)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1} Average loss: {train_loss/len(train_loader.dataset):.4f}')\n",
    "\n",
    "\n",
    "# Train for 20 epochs\n",
    "train_base_sequence = gen_fib_basis(m=20)#gen_korobov_basis(a=76,d=2,n=1021)\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    train(epoch, train_base_sequence.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjcBgC8_P4Wj"
   },
   "outputs": [],
   "source": [
    "# Generate new samples\n",
    "import matplotlib as mpl\n",
    "n_samples_dim = 10\n",
    "n_samples=n_samples_dim**2\n",
    "cmap=mpl.colormaps['plasma']\n",
    "norm = mpl.colors.Normalize(-1,n_samples)\n",
    "with torch.no_grad():\n",
    "    #z = torch.rand(n_samples, 2).to(device)\n",
    "    xx,yy = torch.meshgrid([torch.arange(int(np.sqrt(n_samples)))/int(np.sqrt(n_samples))]*2,indexing='ij')\n",
    "    z = torch.stack([xx.flatten(),yy.flatten()],axis=-1).to(device)\n",
    "    sample = model.decoder(z).detach().cpu()\n",
    "z = z.detach().cpu().numpy()\n",
    "inds = np.arange(n_samples)\n",
    "cs = cmap(norm(inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "jhYGr44CRRT1",
    "outputId": "a29a1f6e-450c-453f-bb52-5c678af1c0b6"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_samples_dim, n_samples_dim, sharey=True, sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes.T.ravel()):\n",
    "    ax.imshow(sample[i, 0, :, :], cmap='grey')\n",
    "    ax.spines[['right','left','top','bottom']].set_color(cmap(norm(i)))\n",
    "    ax.spines[['right','left','top','bottom']].set_linewidth(4)\n",
    "    \n",
    "axes[0, 0].set_xticks([])\n",
    "axes[0, 0].set_yticks([])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.scatter(z[:,0],z[:,1],c=cs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_density(model,eval_grid,sample,batch_size=-1):\n",
    "\n",
    "        decoded = model.decoder(eval_grid %1)\n",
    "        if batch_size == -1:\n",
    "            batch_size = sample.shape[0]\n",
    "\n",
    "        binom_lps = []\n",
    "        for on in range(0,sample.shape[0],batch_size):\n",
    "            off = on + batch_size\n",
    "            \n",
    "            s = sample[on:off].tile(1,decoded.shape[0],1,1)\n",
    "            d = decoded.swapaxes(0,1).tile(s.shape[0],1,1,1)\n",
    "            \n",
    "            binomLP = -1 * binary_cross_entropy(d,\n",
    "                                                s,\n",
    "                                                reduction='none'\n",
    "                                               ).sum(axis=(2,3))\n",
    "            binom_lps.append(binomLP)\n",
    "        binom_lps = torch.cat(binom_lps,axis=0)\n",
    "        return binom_lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "n = 2039\n",
    "a = 1487\n",
    "test_base_sequence = gen_korobov_basis(a=a,d=d,n=n)\n",
    "plot_test_sequence = (test_base_sequence %1).detach().cpu().numpy()\n",
    "\n",
    "data = test_loader.dataset.data.to(torch.float32)\n",
    "labels = test_loader.dataset.targets.detach().cpu().numpy()\n",
    "classes = np.unique(labels)\n",
    "class_densities = {str(l):[] for l in classes}\n",
    "class_images = {str(l):[] for l in classes}\n",
    "for (sample,label) in test_loader:\n",
    "\n",
    "    label = str(label.detach().cpu().numpy().squeeze())\n",
    "    #print(label)\n",
    "    log_density = latent_density(model,test_base_sequence.to(device),sample.to(device),batch_size=1).detach().cpu().numpy()\n",
    "    class_densities[label].append(log_density)\n",
    "    class_images[label].append(sample.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\"\"\"\n",
    "ax = plt.gca()\n",
    "g = ax.scatter(plot_test_sequence[:,0],plot_test_sequence[:,1],c=avg_log_density,cmap='plasma',vmin=-1000,vmax=0)\n",
    "ax.set_title(f\"log density for test {label}\")\n",
    "plt.colorbar(g,ax=ax)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_densities = {c: np.concatenate(class_densities[c],axis=0) for c in class_densities.keys()}\n",
    "class_images = {c: np.stack(class_images[c],axis=0) for c in class_images.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [['0','0','1','1','2','2','3','3','4','4','cb'],\n",
    " ['5','5','6','6','7','7','8','8','9','9','cb']]\n",
    "#fig, axs = plt.subplot_mosaic(mosaic,figsize=(15,5))\n",
    "categorical_cmap = mpl.colormaps['tab10']\n",
    "fig,axs = plt.subplots(nrows=1,ncols=2,width_ratios=(8,1))\n",
    "ls = []\n",
    "for c in class_densities.keys():\n",
    "    dens = class_densities[c]\n",
    "    class_maxs = np.argmax(dens,axis=1)\n",
    "    #print(class_maxs)\n",
    "    avg_dens = np.nanmean(dens,axis=0)\n",
    "    #ax = plt.gca()\n",
    "    #g=axs[c].scatter(plot_test_sequence[:,0],plot_test_sequence[:,1],c=avg_dens,vmin=-1200,vmax=-200)\n",
    "    l = axs[0].scatter(plot_test_sequence[class_maxs,0],plot_test_sequence[class_maxs,1],color=categorical_cmap(int(c)),alpha=0.5,label=c)\n",
    "    ls.append(l)\n",
    "    #ax.set_title(f\"{c}\")\n",
    "#fig.suptitle(\"Average log density across all test samples\")    \n",
    "#fig.suptitle(\"Distribution of projections for each class\")    \n",
    "#cb = plt.colorbar(g,cax=axs['cb'])\n",
    "leg = axs[1].legend(ls,class_densities.keys(),frameon=False)\n",
    "for lh in leg.legend_handles: \n",
    "    lh.set_alpha(1)\n",
    "axs[1].spines[['top','right','left','bottom']].set_visible(False)\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual sample plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=10\n",
    "\n",
    "for c in class_densities.keys():\n",
    "\n",
    "    dens = class_densities[c]\n",
    "    ims = class_images[c]\n",
    "    vmin = np.amin(dens)\n",
    "    vmax = np.amax(dens)\n",
    "    \n",
    "    ss = np.random.choice(len(dens),n_samples,replace=False)\n",
    "    fig,axs = plt.subplots(nrows=2,ncols=n_samples,figsize=(20,8))\n",
    "    for ii,(s,ax,ax2) in enumerate(zip(ss,axs[0,:],axs[1,:])):\n",
    "        #fig,(ax,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "        #ax.scatter(plot_test_sequence[:,0],plot_test_sequence[:,1],c=dens[s])\n",
    "        ax.hexbin(plot_test_sequence[:,0],plot_test_sequence[:,1],C=dens[s],gridsize=25)\n",
    "        ax.spines[['right','top']].set_visible(False)\n",
    "        ax2.imshow(ims[s],cmap='gray')\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_xlim([0,1])\n",
    "        if ii == 0:\n",
    "            ax.set_yticks([0,1])\n",
    "            ax.set_xticks([0,1])\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "    plt.subplots_adjust(hspace=0.0)   \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQk_d5T-RRqq"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "cmap=mpl.colormaps['plasma']\n",
    "norm = mpl.colors.Normalize(0,64)\n",
    "cmap(norm(63))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "consistency-kernel-correct-torch",
   "language": "python",
   "name": "consistency-kernel-correct-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
